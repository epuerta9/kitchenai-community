{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext kitchenai.contrib.cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Project name set to 'my-local-project'.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%kitchenai_set_project llama-index-starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchenai_result: import 'import1' is already registered with the same code.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'msg': 'ran'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%kitchenai_import llama-index-imports\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import os \n",
    "import chromadb\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor)\n",
    "\n",
    "from llama_index.core import Document\n",
    "from kitchenai.contrib.kitchenai_sdk.storage.llama_parser import Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchenai_result: Registered new CodeSetup with hash c9f18e7f89512dc3da10c6418c4dcda3527f3ed6cb510697d1d224f09ebe649f.\n"
     ]
    }
   ],
   "source": [
    "%%kitchenai_setup global_setup\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchenai_result: Registered new CodeSetup with hash 46416472ee8a43347ec8ea0a3c90cdad3cc3d39e2349df0480f38bdd75656794.\n"
     ]
    }
   ],
   "source": [
    "%%kitchenai_setup secondary_setup\n",
    "\n",
    "chroma_collection_second_collection = chroma_client.get_or_create_collection(\"second_collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./data/\"\n",
    "metadata = {}\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%kitchenai_register storage simple-vector\n",
    "\n",
    "\"\"\"\n",
    "Store uploaded files into a vector store with metadata\n",
    "\"\"\"\n",
    "\n",
    "parser = Parser(api_key=os.environ.get(\"LLAMA_CLOUD_API_KEY\", None))\n",
    "\n",
    "response = parser.load(dir, metadata=metadata, **kwargs)\n",
    "\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "# quickstart index\n",
    "VectorStoreIndex.from_documents(\n",
    "    response[\"documents\"], storage_context=storage_context, show_progress=True,\n",
    "        transformations=[TokenTextSplitter(), TitleExtractor(),QuestionsAnsweredExtractor()]\n",
    ")\n",
    "\n",
    "{\"response\": len(response[\"documents\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%kitchenai_register storage simple-vector2\n",
    "\n",
    "\"\"\"\n",
    "Store uploaded files into a vector store with metadata\n",
    "\"\"\"\n",
    "\n",
    "parser = Parser(api_key=os.environ.get(\"LLAMA_CLOUD_API_KEY\", None))\n",
    "\n",
    "response = parser.load(dir, metadata=metadata, **kwargs)\n",
    "\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection_second_collection)\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "# quickstart index\n",
    "VectorStoreIndex.from_documents(\n",
    "    response[\"documents\"], storage_context=storage_context, show_progress=True,\n",
    "        transformations=[TokenTextSplitter(), TitleExtractor(),QuestionsAnsweredExtractor()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitchenai.contrib.kitchenai_sdk.api import QuerySchema\n",
    "\n",
    "data =  QuerySchema(query=\"summarize llama3\", metadata={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%kitchenai_register query simple-query\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    ")\n",
    "query_engine = index.as_query_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
    "response = query_engine.query(data.query)\n",
    "\n",
    "{\"response\": response.response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%kitchenai_register query non-ai\n",
    "\n",
    "msg = \"no AI is used in this function\"\n",
    "\n",
    "{\"response\": msg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitchenai.contrib.kitchenai_sdk.api import EmbedSchema\n",
    "\n",
    "data = EmbedSchema(text=\"this is my first text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchenai_prompt_with_context: Context information is below.\n",
      "---------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "from llama_index.core import VectorStoreIndex, StorageContext\n",
      "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
      "from llama_index.llms.openai import OpenAI\n",
      "import os \n",
      "import chromadb\n",
      "from llama_index.llms.openai import OpenAI\n",
      "from llama_index.core.node_parser import TokenTextSplitter\n",
      "from llama_index.core.extractors import (\n",
      "    TitleExtractor,\n",
      "    QuestionsAnsweredExtractor)\n",
      "\n",
      "from llama_index.core import Document\n",
      "from kitchenai.contrib.kitchenai_sdk.storage.llama_parser import Parser\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "from llama_index.core import VectorStoreIndex, StorageContext\n",
      "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
      "from llama_index.llms.openai import OpenAI\n",
      "import os \n",
      "import chromadb\n",
      "from llama_index.llms.openai import OpenAI\n",
      "from llama_index.core.node_parser import TokenTextSplitter\n",
      "from llama_index.core.extractors import (\n",
      "    TitleExtractor,\n",
      "    QuestionsAnsweredExtractor)\n",
      "\n",
      "from llama_index.core import Document\n",
      "from kitchenai.contrib.kitchenai_sdk.storage.llama_parser import Parser\n",
      "\n",
      "{\"msg\": \"ran\"}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "from llama_index.core import VectorStoreIndex, StorageContext\n",
      "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
      "from llama_index.llms.openai import OpenAI\n",
      "import os \n",
      "import chromadb\n",
      "from llama_index.llms.openai import OpenAI\n",
      "from llama_index.core.node_parser import TokenTextSplitter\n",
      "from llama_index.core.extractors import (\n",
      "    TitleExtractor,\n",
      "    QuestionsAnsweredExtractor)\n",
      "\n",
      "from llama_index.core import Document\n",
      "from kitchenai.contrib.kitchenai_sdk.storage.llama_parser import Parser\n",
      "\n",
      "{\"msg\": \"ran\"}\n",
      "print(\"ok\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "\n",
      "\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "\n",
      "\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "\n",
      "\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "\n",
      "\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "\n",
      "\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "\n",
      "\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_collection_second_collection = chroma_client.get_or_create_collection(\"second_collection\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "\n",
      "\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "\n",
      "\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "\n",
      "\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "\n",
      "\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:GLOBAL_VAR\n",
      "\n",
      "\n",
      "chroma_collection_second_collection = chroma_client.get_or_create_collection(\"second_collection\")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:storage.my-file-label\n",
      "\n",
      "\n",
      "#use the function definition: def my-file-label(dir: str, metadata: dict = {}, *args, **kwargs):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "Store uploaded files into a vector store with metadata\n",
      "\"\"\"\n",
      "\n",
      "parser = Parser(api_key=os.environ.get(\"LLAMA_CLOUD_API_KEY\", None))\n",
      "\n",
      "response = parser.load(dir, metadata=metadata, **kwargs)\n",
      "\n",
      "\n",
      "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
      "\n",
      "# set up ChromaVectorStore and load in data\n",
      "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
      "        \n",
      "# quickstart index\n",
      "VectorStoreIndex.from_documents(\n",
      "    response[\"documents\"], storage_context=storage_context, show_progress=True,\n",
      "        transformations=[TokenTextSplitter(), TitleExtractor(),QuestionsAnsweredExtractor()]\n",
      ")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:query.simple-query\n",
      "\n",
      "\n",
      "#use the function definition: def simple-query(data: QuerySchema):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
      "\n",
      "index = VectorStoreIndex.from_vector_store(\n",
      "    vector_store,\n",
      ")\n",
      "query_engine = index.as_query_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
      "response = query_engine.query(data.query)\n",
      "\n",
      "print(response.response)\n",
      "#return {\"response\": response.response}\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:query.non-ai\n",
      "\n",
      "\n",
      "#use the function definition: def non-ai(data: QuerySchema):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "print(\"no AI is used in this function\")\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:storage.collection2\n",
      "\n",
      "\n",
      "#use the function definition: def collection2(dir: str, metadata: dict = {}, *args, **kwargs):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "Store uploaded files into a vector store with metadata\n",
      "\"\"\"\n",
      "\n",
      "parser = Parser(api_key=os.environ.get(\"LLAMA_CLOUD_API_KEY\", None))\n",
      "\n",
      "response = parser.load(dir, metadata=metadata, **kwargs)\n",
      "\n",
      "\n",
      "vector_store = ChromaVectorStore(chroma_collection=chroma_collection_second_collection)\n",
      "\n",
      "# set up ChromaVectorStore and load in data\n",
      "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
      "        \n",
      "# quickstart index\n",
      "VectorStoreIndex.from_documents(\n",
      "    response[\"documents\"], storage_context=storage_context, show_progress=True,\n",
      "        transformations=[TokenTextSplitter(), TitleExtractor(),QuestionsAnsweredExtractor()]\n",
      ")\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#kitchenai:embedding.simple-embed\n",
      "\n",
      "\n",
      "#use the function definition: def simple-embed(instance: EmbedObject, metadata: dict = {}, **kwargs):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\"ok\": data.text}\n",
      "\n",
      "\n",
      "#kitchenai:end\n",
      "\n",
      "\n",
      "\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, \\\n",
      "convert the given context information into a proper kitchenai application. \\\n",
      "If the context is missing the appropriate kitchenai imports, please add them and keep the remaining. \n",
      "convert the context between kitchen comments.\n",
      "In your final response, just return the python code with no additional comments\n",
      "#kitchenai:<function_type>:<function_label> and \n",
      "\n",
      "#kitchenai:end \n",
      "\n",
      "only convert the <function_type> to the ones that are given. \\ Do not create new function types that are not described in the comments\n",
      "\n",
      "Make sure to include #kitchenai:GLOBAL_VARS as global variables.\n",
      "\n",
      "Please provide a correctly converted kitchenai python app. \\\n",
      "\n",
      "Respond with only the exact content of the answer. Do not include any comments, explanations, code formatting, or additional characters such as quotation marks, markdown, or code blocks. Just output the plain response text.\n",
      "An example of a correct kitchenai app is given below.\n",
      "\n",
      "from kitchenai.contrib.kitchenai_sdk.kitchenai import KitchenAIApp\n",
      "from kitchenai.contrib.kitchenai_sdk.api import QuerySchema, EmbedSchema\n",
      "\n",
      "#This is where all the imports go\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Global variables go here. Make sure to include KitchenAIApp initialization.\n",
      "\n",
      "kitchen = KitchenAIApp()\n",
      "\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\"\"\"\"\n",
      "These are the input and response schemas \n",
      "\n",
      "class QuerySchema(Schema):\n",
      "    query: str\n",
      "    metadata: dict[str, str] | None = None\n",
      "\n",
      "class QueryResponseSchema(Schema):\n",
      "    response: str\n",
      "\n",
      "class AgentResponseSchema(Schema):\n",
      "    response: str\n",
      "\n",
      "class EmbedSchema(Schema):\n",
      "    text: str\n",
      "    metadata: dict[str, str] | None = None\n",
      "\n",
      "\n",
      "\"\"\"\"\n",
      "\n",
      "@kitchen.storage(\"file\")\n",
      "def chromadb_storage(dir: str, metadata: dict = {}, *args, **kwargs):\n",
      "    \"\"\"\n",
      "    Store uploaded files into a vector store with metadata\n",
      "    \"\"\"\n",
      "\n",
      "    #this is where the storage implementation goes. It deals with logic around processing a temporary file placed in the dir argument.\n",
      "\n",
      "\n",
      "    return a dictionary with results of the storage process\n",
      "\n",
      "@kitchen.embed(\"embed\")\n",
      "def embed(text: EmbedSchema, metadata: dict = {}, **kwargs):\n",
      "    \"\"\"Embed single pieces of text\"\"\"\n",
      "\n",
      "    return a dictionary with the results of the embedding process\n",
      "\n",
      "@kitchen.query(\"simple-query\")\n",
      "async def query(data: QuerySchema):\n",
      "    #Function to test query against your vectorized database.\n",
      "\n",
      "    return a QueryResponseSchema\n",
      "\n",
      "\n",
      "@kitchen.query(\"chat\")\n",
      "async def query_chat(data: QuerySchema):\n",
      "    #Use can have multiple functions of the same type given a different label\n",
      "\n",
      "    return a QueryResponseSchema\n",
      "\n",
      "\n",
      "@kitchen.agent(\"agent-create\")\n",
      "def agent_create(data: QuerySchema):\n",
      "\n",
      "    #Agent functions act the same as query giving the user the ability to define Agent related functionality here.\n",
      "\n",
      "    return an AgentResponseSchema\n",
      "\n",
      "\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchenai_response: from kitchenai.contrib.kitchenai_sdk.kitchenai import KitchenAIApp\n",
      "from kitchenai.contrib.kitchenai_sdk.api import QuerySchema, EmbedSchema\n",
      "from llama_index.core import VectorStoreIndex, StorageContext\n",
      "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
      "from llama_index.llms.openai import OpenAI\n",
      "import os \n",
      "import chromadb\n",
      "from llama_index.core.node_parser import TokenTextSplitter\n",
      "from llama_index.core.extractors import (\n",
      "    TitleExtractor,\n",
      "    QuestionsAnsweredExtractor)\n",
      "from llama_index.core import Document\n",
      "from kitchenai.contrib.kitchenai_sdk.storage.llama_parser import Parser\n",
      "\n",
      "kitchen = KitchenAIApp()\n",
      "\n",
      "chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
      "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "chroma_collection_second_collection = chroma_client.get_or_create_collection(\"second_collection\")\n",
      "\n",
      "@kitchen.storage(\"my-file-label\")\n",
      "def my_file_label(dir: str, metadata: dict = {}, *args, **kwargs):\n",
      "    parser = Parser(api_key=os.environ.get(\"LLAMA_CLOUD_API_KEY\", None))\n",
      "    response = parser.load(dir, metadata=metadata, **kwargs)\n",
      "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
      "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
      "    VectorStoreIndex.from_documents(\n",
      "        response[\"documents\"], storage_context=storage_context, show_progress=True,\n",
      "            transformations=[TokenTextSplitter(), TitleExtractor(),QuestionsAnsweredExtractor()]\n",
      "    )\n",
      "\n",
      "@kitchen.query(\"simple-query\")\n",
      "async def simple_query(data: QuerySchema):\n",
      "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
      "    index = VectorStoreIndex.from_vector_store(vector_store)\n",
      "    query_engine = index.as_query_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
      "    response = query_engine.query(data.query)\n",
      "    return {\"response\": response.response}\n",
      "\n",
      "@kitchen.query(\"non-ai\")\n",
      "async def non_ai(data: QuerySchema):\n",
      "    return {\"response\": \"no AI is used in this function\"}\n",
      "\n",
      "@kitchen.storage(\"collection2\")\n",
      "def collection2(dir: str, metadata: dict = {}, *args, **kwargs):\n",
      "    parser = Parser(api_key=os.environ.get(\"LLAMA_CLOUD_API_KEY\", None))\n",
      "    response = parser.load(dir, metadata=metadata, **kwargs)\n",
      "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection_second_collection)\n",
      "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
      "    VectorStoreIndex.from_documents(\n",
      "        response[\"documents\"], storage_context=storage_context, show_progress=True,\n",
      "            transformations=[TokenTextSplitter(), TitleExtractor(),QuestionsAnsweredExtractor()]\n",
      "    )\n",
      "\n",
      "@kitchen.embed(\"simple-embed\")\n",
      "def simple_embed(instance: EmbedSchema, metadata: dict = {}, **kwargs):\n",
      "    return {\"ok\": instance.text}\n",
      "--------------------------------\n",
      "kitchenai_result: Created app.py\n"
     ]
    }
   ],
   "source": [
    "%kitchenai_create_module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
